{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\snowflake\\sqlalchemy\\base.py:1068: SAWarning: The GenericFunction 'flatten' is already registered and is going to be overridden.\n",
      "  functions.register_function(\"flatten\", flatten)\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import os\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "logging.basicConfig(filename='etl_pipeline.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging to be less detailed and add extra space between logs\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='ETL_pipeline.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s\\n\\n'  # Adjust the format as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure the logging to be less detailed and add extra space between logs\n",
    "logging.basicConfig(\n",
    "    filename='ETL_pipeline.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s\\n\\n'  # Adjust the format as needed\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "logging.info('This is an informational message.')\n",
    "logging.error('This is an error message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' account_name = os.getenv('AZURE_ACCOUNT_NAME')\n",
    "account_key = os.getenv('AZURE_ACCOUNT_KEY')\n",
    "smtp_server = os.getenv('SMTP_SERVER')\n",
    "smtp_port = int(os.getenv('SMTP_PORT'))\n",
    "email_sender = os.getenv('EMAIL_SENDER')\n",
    "email_recipient = os.getenv('EMAIL_RECIPIENT') '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADLS BlobServiceClient\n",
    "\n",
    "account_name = os.getenv('AzureAN')\n",
    "account_key = os.getenv('AzureAK')\n",
    "container_name = os.getenv('AzureCN')\n",
    "\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Data and moving to another folder\n",
    "\n",
    "def move_blob(blob_name, destination_folder):\n",
    "    try:\n",
    "        logging.info(f'Moving {blob_name} to {destination_folder}.')\n",
    "        \n",
    "        # Create the destination blob path\n",
    "        destination_blob_name = f'{\"payroll-extracteddata\"}/{blob_name.split(\"/\")[-1]}'  # Placeholder: 'destination_folder' should be the folder name where you want to move processed files\n",
    "        \n",
    "        # Get the source and destination blob clients\n",
    "        source_blob = container_client.get_blob_client(blob_name)\n",
    "        destination_blob = container_client.get_blob_client(destination_blob_name)\n",
    "\n",
    "        # Copy the blob to the destination\n",
    "        destination_blob.start_copy_from_url(source_blob.url)\n",
    "        \n",
    "        # Delete the original blob\n",
    "        source_blob.delete_blob()\n",
    "\n",
    "        logging.info(f'Moved {blob_name} to {destination_folder}.')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to move {blob_name}: {e}')\n",
    "\n",
    "\n",
    "\n",
    "def extract_files_from_adls(destination_folder):\n",
    "    try:\n",
    "        # List blobs with a specific prefix (Placeholder: 'new-data/' should be replaced with the prefix of your new data folder)\n",
    "        blobs_list = container_client.list_blobs(name_starts_with=\"payroll-sourcedata/\")\n",
    "        \n",
    "        for blob in blobs_list:\n",
    "            blob_name = blob.name\n",
    "            \n",
    "            # Define the local download path\n",
    "            download_path = os.path.join(\"Raw_payroll_Data\", blob_name.split('/')[-1])  # Placeholder: 'destination_folder' should be the local directory where files will be downloaded\n",
    "            \n",
    "            logging.info(f'Downloading {blob_name} from ADLS to {download_path}.')\n",
    "            \n",
    "            # Download the blob to the local path\n",
    "            blob_client = container_client.get_blob_client(blob_name)\n",
    "            with open(download_path, \"wb\") as file:\n",
    "                file.write(blob_client.download_blob().readall())\n",
    "                \n",
    "            logging.info(f'Downloaded {blob_name} successfully.')\n",
    "            \n",
    "            # Move the blob to the processed-data folder after downloading (Placeholder: 'processed-data' should be the folder where you want to move processed files)\n",
    "            move_blob(blob_name, 'payroll-extracteddata')\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error extracting files from ADLS: {e}')\n",
    "\n",
    "\n",
    "extract_files_from_adls('Raw_payroll_Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found blobs: ['payroll-extracteddata', 'payroll-sourcedata', 'payroll-sourcedata/nycpayroll_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "# To check the directory/folders in the specified container\n",
    "\n",
    "blobs_list = container_client.list_blobs()\n",
    "\n",
    "print(f'Found blobs: {[blob.name for blob in blobs_list]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: read_csv() got an unexpected keyword argument 'infer_objects'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:771\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[1;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[0;32m    759\u001b[0m     urlpath,\n\u001b[0;32m    760\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    770\u001b[0m ):\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_pandas(\n\u001b[0;32m    772\u001b[0m         reader,\n\u001b[0;32m    773\u001b[0m         urlpath,\n\u001b[0;32m    774\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[0;32m    775\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m    776\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    777\u001b[0m         sample\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m    778\u001b[0m         sample_rows\u001b[38;5;241m=\u001b[39msample_rows,\n\u001b[0;32m    779\u001b[0m         enforce\u001b[38;5;241m=\u001b[39menforce,\n\u001b[0;32m    780\u001b[0m         assume_missing\u001b[38;5;241m=\u001b[39massume_missing,\n\u001b[0;32m    781\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    782\u001b[0m         include_path_column\u001b[38;5;241m=\u001b[39minclude_path_column,\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    784\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:640\u001b[0m, in \u001b[0;36mread_pandas\u001b[1;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     head \u001b[38;5;241m=\u001b[39m reader(BytesIO(b_sample), nrows\u001b[38;5;241m=\u001b[39msample_rows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhead_kwargs)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pd\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mParserError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'infer_objects'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Converting the data to dask dataframe\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m NYCpayroll_df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRaw_payroll_Data\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mnycpayroll_2020.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:5115\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(path, header, dtype_backend, storage_options, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   5114\u001b[0m     path \u001b[38;5;241m=\u001b[39m stringify_path(path)\n\u001b[1;32m-> 5115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mReadCSV\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataframe_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:4794\u001b[0m, in \u001b[0;36mnew_collection\u001b[1;34m(expr)\u001b[0m\n\u001b[0;32m   4792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_collection\u001b[39m(expr):\n\u001b[0;32m   4793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4794\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\n\u001b[0;32m   4795\u001b[0m     expr\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\io\\csv.py:92\u001b[0m, in \u001b[0;36mReadCSV._meta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ddf\u001b[49m\u001b[38;5;241m.\u001b[39m_meta\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\io\\csv.py:82\u001b[0m, in \u001b[0;36mReadCSV._ddf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m usecols:\n\u001b[0;32m     80\u001b[0m     columns \u001b[38;5;241m=\u001b[39m usecols\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation(\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[0;32m     84\u001b[0m     usecols\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m     85\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m     86\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     88\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\backends.py:151\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: read_csv() got an unexpected keyword argument 'infer_objects'"
     ]
    }
   ],
   "source": [
    "# Converting the data to dask dataframe\n",
    "\n",
    "NYCpayroll_df = dd.read_csv(r\"Raw_payroll_Data\\nycpayroll_2020.csv\", infer_objects=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [WinError 2] The system cannot find the file specified: 'c:/Users/oluwa/OneDrive/Documents/Data Engineering Training - 10Alytics/Capstone Project/NYC_Payroll_Pipeline/nycpayroll.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:771\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[1;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[0;32m    759\u001b[0m     urlpath,\n\u001b[0;32m    760\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    770\u001b[0m ):\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_pandas(\n\u001b[0;32m    772\u001b[0m         reader,\n\u001b[0;32m    773\u001b[0m         urlpath,\n\u001b[0;32m    774\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[0;32m    775\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m    776\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    777\u001b[0m         sample\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m    778\u001b[0m         sample_rows\u001b[38;5;241m=\u001b[39msample_rows,\n\u001b[0;32m    779\u001b[0m         enforce\u001b[38;5;241m=\u001b[39menforce,\n\u001b[0;32m    780\u001b[0m         assume_missing\u001b[38;5;241m=\u001b[39massume_missing,\n\u001b[0;32m    781\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    782\u001b[0m         include_path_column\u001b[38;5;241m=\u001b[39minclude_path_column,\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    784\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:571\u001b[0m, in \u001b[0;36mread_pandas\u001b[1;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m     sample \u001b[38;5;241m=\u001b[39m blocksize\n\u001b[1;32m--> 571\u001b[0m b_out \u001b[38;5;241m=\u001b[39m read_bytes(\n\u001b[0;32m    572\u001b[0m     urlpath,\n\u001b[0;32m    573\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39mb_lineterminator,\n\u001b[0;32m    574\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[0;32m    575\u001b[0m     sample\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m    576\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    577\u001b[0m     include_path\u001b[38;5;241m=\u001b[39minclude_path_column,\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    579\u001b[0m )\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_path_column:\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\bytes\\core.py:111\u001b[0m, in \u001b[0;36mread_bytes\u001b[1;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, include_path, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot do chunked reads on compressed files. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo read, set blocksize=None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m     )\n\u001b[1;32m--> 111\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\fsspec\\implementations\\local.py:86\u001b[0m, in \u001b[0;36mLocalFileSystem.info\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m---> 86\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m link \u001b[38;5;241m=\u001b[39m stat\u001b[38;5;241m.\u001b[39mS_ISLNK(out\u001b[38;5;241m.\u001b[39mst_mode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'c:/Users/oluwa/OneDrive/Documents/Data Engineering Training - 10Alytics/Capstone Project/NYC_Payroll_Pipeline/nycpayroll.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nycpayroll_df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnycpayroll.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:5115\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(path, header, dtype_backend, storage_options, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   5114\u001b[0m     path \u001b[38;5;241m=\u001b[39m stringify_path(path)\n\u001b[1;32m-> 5115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mReadCSV\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataframe_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:4794\u001b[0m, in \u001b[0;36mnew_collection\u001b[1;34m(expr)\u001b[0m\n\u001b[0;32m   4792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_collection\u001b[39m(expr):\n\u001b[0;32m   4793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4794\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\n\u001b[0;32m   4795\u001b[0m     expr\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\io\\csv.py:92\u001b[0m, in \u001b[0;36mReadCSV._meta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ddf\u001b[49m\u001b[38;5;241m.\u001b[39m_meta\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\io\\csv.py:82\u001b[0m, in \u001b[0;36mReadCSV._ddf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m usecols:\n\u001b[0;32m     80\u001b[0m     columns \u001b[38;5;241m=\u001b[39m usecols\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation(\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[0;32m     84\u001b[0m     usecols\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m     85\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m     86\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     88\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\backends.py:151\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [WinError 2] The system cannot find the file specified: 'c:/Users/oluwa/OneDrive/Documents/Data Engineering Training - 10Alytics/Capstone Project/NYC_Payroll_Pipeline/nycpayroll.csv'"
     ]
    }
   ],
   "source": [
    "nycpayroll_df = dd.read_csv('nycpayroll.csv', infer_objects=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 19 entries, FiscalYear to TotalOtherPay\n",
      "dtypes: float64(3), int64(8), string(8)"
     ]
    }
   ],
   "source": [
    "# Data Transformation\n",
    "\n",
    "\n",
    "NYCpayroll_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------------+---------+----------+\n| Column       | Found   | Expected |\n+--------------+---------+----------+\n| BaseSalary   | float64 | int64    |\n| OTHours      | float64 | int64    |\n| RegularHours | float64 | int64    |\n+--------------+---------+----------+\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'BaseSalary': 'float64',\n       'OTHours': 'float64',\n       'RegularHours': 'float64'}\n\nto the call to `read_csv`/`read_table`.\n\nAlternatively, provide `assume_missing=True` to interpret\nall unspecified integer columns as floats.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mNYCpayroll_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:703\u001b[0m, in \u001b[0;36mFrameBase.head\u001b[1;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[0;32m    701\u001b[0m out \u001b[38;5;241m=\u001b[39m new_collection(expr\u001b[38;5;241m.\u001b[39mHead(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39mn, npartitions\u001b[38;5;241m=\u001b[39mnpartitions))\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m--> 703\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:477\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    476\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_expr.py:3758\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[1;34m(graph, name, *deps)\u001b[0m\n\u001b[0;32m   3756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[0;32m   3757\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[1;32m-> 3758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[1;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[1;32m--> 197\u001b[0m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[1;34m(df, dtypes)\u001b[0m\n\u001b[0;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[0;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------------+---------+----------+\n| Column       | Found   | Expected |\n+--------------+---------+----------+\n| BaseSalary   | float64 | int64    |\n| OTHours      | float64 | int64    |\n| RegularHours | float64 | int64    |\n+--------------+---------+----------+\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'BaseSalary': 'float64',\n       'OTHours': 'float64',\n       'RegularHours': 'float64'}\n\nto the call to `read_csv`/`read_table`.\n\nAlternatively, provide `assume_missing=True` to interpret\nall unspecified integer columns as floats."
     ]
    }
   ],
   "source": [
    "NYCpayroll_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYCpayroll_df = dd.read_csv(r\"Raw_payroll_Data\\nycpayroll_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------------+---------+----------+\n| Column       | Found   | Expected |\n+--------------+---------+----------+\n| BaseSalary   | float64 | int64    |\n| OTHours      | float64 | int64    |\n| RegularHours | float64 | int64    |\n+--------------+---------+----------+\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'BaseSalary': 'float64',\n       'OTHours': 'float64',\n       'RegularHours': 'float64'}\n\nto the call to `read_csv`/`read_table`.\n\nAlternatively, provide `assume_missing=True` to interpret\nall unspecified integer columns as floats.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mNYCpayroll_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:703\u001b[0m, in \u001b[0;36mFrameBase.head\u001b[1;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[0;32m    701\u001b[0m out \u001b[38;5;241m=\u001b[39m new_collection(expr\u001b[38;5;241m.\u001b[39mHead(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39mn, npartitions\u001b[38;5;241m=\u001b[39mnpartitions))\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m--> 703\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_collection.py:477\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    476\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask_expr\\_expr.py:3758\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[1;34m(graph, name, *deps)\u001b[0m\n\u001b[0;32m   3756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[0;32m   3757\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[1;32m-> 3758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[1;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[1;32m--> 197\u001b[0m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[1;32mc:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[1;34m(df, dtypes)\u001b[0m\n\u001b[0;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[0;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------------+---------+----------+\n| Column       | Found   | Expected |\n+--------------+---------+----------+\n| BaseSalary   | float64 | int64    |\n| OTHours      | float64 | int64    |\n| RegularHours | float64 | int64    |\n+--------------+---------+----------+\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'BaseSalary': 'float64',\n       'OTHours': 'float64',\n       'RegularHours': 'float64'}\n\nto the call to `read_csv`/`read_table`.\n\nAlternatively, provide `assume_missing=True` to interpret\nall unspecified integer columns as floats."
     ]
    }
   ],
   "source": [
    "NYCpayroll_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"FiscalYear\" : \"int64\",\n",
    "    \"PayrollNumber\" : \"int64\",\n",
    "    \"AgencyID\" : \"int64\",\n",
    "    \"AgencyName\" : \"object\",\n",
    "    \"EmployeeID\" : \"int64\",\n",
    "    \"LastName\" : \"object\",\n",
    "    \"FirstName\"  : \"object\",\n",
    "    \"AgencyStartDate\" : \"object\",\n",
    "    \"WorkLocationBorough\" : \"object\", \n",
    "    \"TitleCode\" : \"int64\",\n",
    "   \"TitleDescription\" : \"object\",\n",
    "   \"LeaveStatusasofJune30\" : \"object\",\n",
    "   \"BaseSalary\" : \"float64\",\n",
    "   \"PayBasis\" : \"object\" ,\n",
    "  \"RegularHours\" : \"float64\",\n",
    "   \"RegularGrossPaid\" : \"float64\",\n",
    "   \"OTHours\" : \"float64\",\n",
    "   \"TotalOTPaid\" : \"float64\",\n",
    "   \"TotalOtherPay\" : \"float64\"\n",
    "}\n",
    "\n",
    "NYCpayroll_df = dd.read_csv(r\"Dataset\\Raw_payroll_Data\\nycpayroll_2020.csv\", dtype=dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>PayrollNumber</th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>AgencyName</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>AgencyStartDate</th>\n",
       "      <th>WorkLocationBorough</th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>TitleDescription</th>\n",
       "      <th>LeaveStatusasofJune30</th>\n",
       "      <th>BaseSalary</th>\n",
       "      <th>PayBasis</th>\n",
       "      <th>RegularHours</th>\n",
       "      <th>RegularGrossPaid</th>\n",
       "      <th>OTHours</th>\n",
       "      <th>TotalOTPaid</th>\n",
       "      <th>TotalOtherPay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>10001</td>\n",
       "      <td>GEAGER</td>\n",
       "      <td>VERONICA</td>\n",
       "      <td>9/12/2016</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>149612</td>\n",
       "      <td>ROTTA</td>\n",
       "      <td>JONATHAN</td>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>206583</td>\n",
       "      <td>WILSON II</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>4/30/2018</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>199874</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>MORIAH</td>\n",
       "      <td>3/18/2019</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>87900.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3202.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>58036</td>\n",
       "      <td>KRAWCZYK</td>\n",
       "      <td>AMANDA</td>\n",
       "      <td>5/15/2017</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>83976.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FiscalYear  PayrollNumber  AgencyID                      AgencyName  \\\n",
       "0        2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "1        2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "2        2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "3        2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "4        2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "\n",
       "   EmployeeID    LastName FirstName AgencyStartDate WorkLocationBorough  \\\n",
       "0       10001      GEAGER  VERONICA       9/12/2016            BROOKLYN   \n",
       "1      149612       ROTTA  JONATHAN       9/16/2013            BROOKLYN   \n",
       "2      206583   WILSON II    ROBERT       4/30/2018            BROOKLYN   \n",
       "3      199874  WASHINGTON    MORIAH       3/18/2019            BROOKLYN   \n",
       "4       58036    KRAWCZYK    AMANDA       5/15/2017            BROOKLYN   \n",
       "\n",
       "   TitleCode                TitleDescription LeaveStatusasofJune30  \\\n",
       "0      40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "1      40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "2      40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "3      40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "4      40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "\n",
       "   BaseSalary   PayBasis  RegularHours  RegularGrossPaid  OTHours  \\\n",
       "0     86005.0  per Annum        1820.0          84698.21      0.0   \n",
       "1     86005.0  per Annum        1820.0          84698.21      0.0   \n",
       "2     86005.0  per Annum        1820.0          84698.21      0.0   \n",
       "3     86005.0  per Annum        1820.0          87900.95      0.0   \n",
       "4     86005.0  per Annum        1820.0          83976.54      0.0   \n",
       "\n",
       "   TotalOTPaid  TotalOtherPay  \n",
       "0          0.0           0.00  \n",
       "1          0.0           0.00  \n",
       "2          0.0           0.00  \n",
       "3          0.0       -3202.74  \n",
       "4          0.0           0.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYCpayroll_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 19 entries, FiscalYear to TotalOtherPay\n",
      "dtypes: float64(6), int64(5), string(8)"
     ]
    }
   ],
   "source": [
    "NYCpayroll_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>PayrollNumber</th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>BaseSalary</th>\n",
       "      <th>RegularHours</th>\n",
       "      <th>RegularGrossPaid</th>\n",
       "      <th>OTHours</th>\n",
       "      <th>TotalOTPaid</th>\n",
       "      <th>TotalOtherPay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019.78</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>2120.260000</td>\n",
       "      <td>127735.740000</td>\n",
       "      <td>40407.590000</td>\n",
       "      <td>94344.419900</td>\n",
       "      <td>1164.937500</td>\n",
       "      <td>61417.815700</td>\n",
       "      <td>72.780000</td>\n",
       "      <td>3355.041500</td>\n",
       "      <td>5342.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.20</td>\n",
       "      <td>0.675995</td>\n",
       "      <td>0.675995</td>\n",
       "      <td>56164.043495</td>\n",
       "      <td>95.128313</td>\n",
       "      <td>55375.949822</td>\n",
       "      <td>713.239513</td>\n",
       "      <td>57858.174131</td>\n",
       "      <td>109.869098</td>\n",
       "      <td>5006.887007</td>\n",
       "      <td>22425.288045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1998.00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2120.000000</td>\n",
       "      <td>10001.000000</td>\n",
       "      <td>40081.000000</td>\n",
       "      <td>28.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3202.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2020.00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2120.000000</td>\n",
       "      <td>92662.750000</td>\n",
       "      <td>40447.000000</td>\n",
       "      <td>66603.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>21620.255000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020.00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2120.000000</td>\n",
       "      <td>136026.500000</td>\n",
       "      <td>40448.000000</td>\n",
       "      <td>73765.500000</td>\n",
       "      <td>1431.500000</td>\n",
       "      <td>56058.285000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.510000</td>\n",
       "      <td>223.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2020.00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2120.000000</td>\n",
       "      <td>176790.000000</td>\n",
       "      <td>40448.000000</td>\n",
       "      <td>90361.250000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>70199.420000</td>\n",
       "      <td>124.937500</td>\n",
       "      <td>5417.012500</td>\n",
       "      <td>1323.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.00</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2122.000000</td>\n",
       "      <td>212044.000000</td>\n",
       "      <td>40494.000000</td>\n",
       "      <td>243171.000000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>239475.950000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>19751.620000</td>\n",
       "      <td>180605.850000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FiscalYear  PayrollNumber     AgencyID     EmployeeID     TitleCode  \\\n",
       "count      100.00     100.000000   100.000000     100.000000    100.000000   \n",
       "mean      2019.78      17.260000  2120.260000  127735.740000  40407.590000   \n",
       "std          2.20       0.675995     0.675995   56164.043495     95.128313   \n",
       "min       1998.00      17.000000  2120.000000   10001.000000  40081.000000   \n",
       "25%       2020.00      17.000000  2120.000000   92662.750000  40447.000000   \n",
       "50%       2020.00      17.000000  2120.000000  136026.500000  40448.000000   \n",
       "75%       2020.00      17.000000  2120.000000  176790.000000  40448.000000   \n",
       "max       2020.00      19.000000  2122.000000  212044.000000  40494.000000   \n",
       "\n",
       "          BaseSalary  RegularHours  RegularGrossPaid     OTHours  \\\n",
       "count     100.000000    100.000000        100.000000  100.000000   \n",
       "mean    94344.419900   1164.937500      61417.815700   72.780000   \n",
       "std     55375.949822    713.239513      57858.174131  109.869098   \n",
       "min        28.840000      0.000000          0.000000    0.000000   \n",
       "25%     66603.000000    385.000000      21620.255000    0.000000   \n",
       "50%     73765.500000   1431.500000      56058.285000    1.000000   \n",
       "75%     90361.250000   1820.000000      70199.420000  124.937500   \n",
       "max    243171.000000   1820.000000     239475.950000  526.000000   \n",
       "\n",
       "        TotalOTPaid  TotalOtherPay  \n",
       "count    100.000000     100.000000  \n",
       "mean    3355.041500    5342.134200  \n",
       "std     5006.887007   22425.288045  \n",
       "min        0.000000   -3202.740000  \n",
       "25%        0.000000       0.000000  \n",
       "50%       62.510000     223.210000  \n",
       "75%     5417.012500    1323.625000  \n",
       "max    19751.620000  180605.850000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYCpayroll_df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID',\n",
       "       'LastName', 'FirstName', 'AgencyStartDate', 'WorkLocationBorough',\n",
       "       'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30', 'BaseSalary',\n",
       "       'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours',\n",
       "       'TotalOTPaid', 'TotalOtherPay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYCpayroll_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FiscalYear               100\n",
       "PayrollNumber            100\n",
       "AgencyID                 100\n",
       "AgencyName               100\n",
       "EmployeeID               100\n",
       "LastName                 100\n",
       "FirstName                100\n",
       "AgencyStartDate          100\n",
       "WorkLocationBorough      100\n",
       "TitleCode                100\n",
       "TitleDescription         100\n",
       "LeaveStatusasofJune30    100\n",
       "BaseSalary               100\n",
       "PayBasis                 100\n",
       "RegularHours             100\n",
       "RegularGrossPaid         100\n",
       "OTHours                  100\n",
       "TotalOTPaid              100\n",
       "TotalOtherPay            100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check for null values and count the number of rows in each column \n",
    "\n",
    "NYCpayroll_df.isnull().compute().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 01:45:57,647  INFO  Starting data transformation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 01:45:57,742  INFO  Data transformation completed successfully.\n",
      "2024-08-24 01:45:57,889  INFO  Transformed data saved to Dataset/Cleaned_payroll_Data\\nycpayroll_2020.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>PayrollNumber</th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>AgencyName</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>AgencyStartDate</th>\n",
       "      <th>WorkLocationBorough</th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>TitleDescription</th>\n",
       "      <th>LeaveStatusasofJune30</th>\n",
       "      <th>BaseSalary</th>\n",
       "      <th>PayBasis</th>\n",
       "      <th>RegularHours</th>\n",
       "      <th>RegularGrossPaid</th>\n",
       "      <th>OTHours</th>\n",
       "      <th>TotalOTPaid</th>\n",
       "      <th>TotalOtherPay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>10001</td>\n",
       "      <td>GEAGER</td>\n",
       "      <td>VERONICA</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>149612</td>\n",
       "      <td>ROTTA</td>\n",
       "      <td>JONATHAN</td>\n",
       "      <td>2013-09-16</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>206583</td>\n",
       "      <td>WILSON II</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>199874</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>MORIAH</td>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>87900.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3202.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "      <td>58036</td>\n",
       "      <td>KRAWCZYK</td>\n",
       "      <td>AMANDA</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>83976.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2020</td>\n",
       "      <td>19</td>\n",
       "      <td>2122</td>\n",
       "      <td>OFFICE OF MANAGEMENT &amp; BUDGET</td>\n",
       "      <td>33054</td>\n",
       "      <td>HIGGINS</td>\n",
       "      <td>JAMES</td>\n",
       "      <td>1982-06-21</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40306</td>\n",
       "      <td>COMPUTER SYSTEMS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>195826.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>192850.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2020</td>\n",
       "      <td>19</td>\n",
       "      <td>2122</td>\n",
       "      <td>OFFICE OF MANAGEMENT &amp; BUDGET</td>\n",
       "      <td>19997</td>\n",
       "      <td>GREENBERG</td>\n",
       "      <td>DAVID</td>\n",
       "      <td>2008-07-14</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40199</td>\n",
       "      <td>BUDGET ANALYST</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>196950.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>192107.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2020</td>\n",
       "      <td>19</td>\n",
       "      <td>2122</td>\n",
       "      <td>OFFICE OF MANAGEMENT &amp; BUDGET</td>\n",
       "      <td>61906</td>\n",
       "      <td>LARSON</td>\n",
       "      <td>CHRISTINE</td>\n",
       "      <td>2005-12-25</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40199</td>\n",
       "      <td>BUDGET ANALYST</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>185843.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>183019.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2020</td>\n",
       "      <td>19</td>\n",
       "      <td>2122</td>\n",
       "      <td>OFFICE OF MANAGEMENT &amp; BUDGET</td>\n",
       "      <td>19003</td>\n",
       "      <td>GRATHWOL</td>\n",
       "      <td>JOHN</td>\n",
       "      <td>1988-09-27</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40199</td>\n",
       "      <td>BUDGET ANALYST</td>\n",
       "      <td>CEASED</td>\n",
       "      <td>216431.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180605.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1998</td>\n",
       "      <td>19</td>\n",
       "      <td>2122</td>\n",
       "      <td>OFFICE OF MANAGEMENT &amp; BUDGET</td>\n",
       "      <td>15188</td>\n",
       "      <td>GOLDSTEIN</td>\n",
       "      <td>JOSHUA</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40199</td>\n",
       "      <td>BUDGET ANALYST</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>176785.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>174098.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FiscalYear  PayrollNumber  AgencyID                      AgencyName  \\\n",
       "0         2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "1         2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "2         2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "3         2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "4         2020             17      2120  OFFICE OF EMERGENCY MANAGEMENT   \n",
       "..         ...            ...       ...                             ...   \n",
       "95        2020             19      2122   OFFICE OF MANAGEMENT & BUDGET   \n",
       "96        2020             19      2122   OFFICE OF MANAGEMENT & BUDGET   \n",
       "97        2020             19      2122   OFFICE OF MANAGEMENT & BUDGET   \n",
       "98        2020             19      2122   OFFICE OF MANAGEMENT & BUDGET   \n",
       "99        1998             19      2122   OFFICE OF MANAGEMENT & BUDGET   \n",
       "\n",
       "    EmployeeID    LastName  FirstName AgencyStartDate WorkLocationBorough  \\\n",
       "0        10001      GEAGER   VERONICA      2016-09-12            BROOKLYN   \n",
       "1       149612       ROTTA   JONATHAN      2013-09-16            BROOKLYN   \n",
       "2       206583   WILSON II     ROBERT      2018-04-30            BROOKLYN   \n",
       "3       199874  WASHINGTON     MORIAH      2019-03-18            BROOKLYN   \n",
       "4        58036    KRAWCZYK     AMANDA      2017-05-15            BROOKLYN   \n",
       "..         ...         ...        ...             ...                 ...   \n",
       "95       33054     HIGGINS      JAMES      1982-06-21           MANHATTAN   \n",
       "96       19997   GREENBERG      DAVID      2008-07-14           MANHATTAN   \n",
       "97       61906      LARSON  CHRISTINE      2005-12-25           MANHATTAN   \n",
       "98       19003    GRATHWOL       JOHN      1988-09-27           MANHATTAN   \n",
       "99       15188   GOLDSTEIN     JOSHUA      1999-01-04           MANHATTAN   \n",
       "\n",
       "    TitleCode                TitleDescription LeaveStatusasofJune30  \\\n",
       "0       40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "1       40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "2       40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "3       40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "4       40447  EMERGENCY PREPAREDNESS MANAGER                ACTIVE   \n",
       "..        ...                             ...                   ...   \n",
       "95      40306        COMPUTER SYSTEMS MANAGER                ACTIVE   \n",
       "96      40199                  BUDGET ANALYST                ACTIVE   \n",
       "97      40199                  BUDGET ANALYST                ACTIVE   \n",
       "98      40199                  BUDGET ANALYST                CEASED   \n",
       "99      40199                  BUDGET ANALYST                ACTIVE   \n",
       "\n",
       "    BaseSalary   PayBasis  RegularHours  RegularGrossPaid  OTHours  \\\n",
       "0      86005.0  per Annum        1820.0          84698.21      0.0   \n",
       "1      86005.0  per Annum        1820.0          84698.21      0.0   \n",
       "2      86005.0  per Annum        1820.0          84698.21      0.0   \n",
       "3      86005.0  per Annum        1820.0          87900.95      0.0   \n",
       "4      86005.0  per Annum        1820.0          83976.54      0.0   \n",
       "..         ...        ...           ...               ...      ...   \n",
       "95    195826.0  per Annum        1820.0         192850.37      0.0   \n",
       "96    196950.0  per Annum        1820.0         192107.94      0.0   \n",
       "97    185843.0  per Annum        1820.0         183019.14      0.0   \n",
       "98    216431.0  per Annum           0.0              0.00      0.0   \n",
       "99    176785.0  per Annum        1820.0         174098.86      0.0   \n",
       "\n",
       "    TotalOTPaid  TotalOtherPay  \n",
       "0           0.0           0.00  \n",
       "1           0.0           0.00  \n",
       "2           0.0           0.00  \n",
       "3           0.0       -3202.74  \n",
       "4           0.0           0.00  \n",
       "..          ...            ...  \n",
       "95          0.0           0.00  \n",
       "96          0.0         500.00  \n",
       "97          0.0           0.00  \n",
       "98          0.0      180605.85  \n",
       "99          0.0           0.00  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s  %(levelname)s  %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"etl_pipeline.log\"),  # Logs to a file named transformation.log\n",
    "        logging.StreamHandler()  # Also prints logs to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform_data(input_file_path, output_file_path): \n",
    "    try:\n",
    "        logging.info(\"Starting data transformation.\")\n",
    "\n",
    "        # Define data types for columns\n",
    "        dtypes = {\n",
    "            \"FiscalYear\" : \"int64\",\n",
    "            \"PayrollNumber\" : \"int64\",\n",
    "            \"AgencyID\" : \"int64\",\n",
    "            \"AgencyName\": \"object\",\n",
    "            \"EmployeeID\": \"int64\",\n",
    "            \"LastName\": \"object\",\n",
    "            \"FirstName\": \"object\",\n",
    "            \"AgencyStartDate\": \"object\",\n",
    "            \"WorkLocationBorough\": \"object\",\n",
    "            \"TitleCode\": \"int64\",\n",
    "            \"TitleDescription\": \"object\",\n",
    "            \"LeaveStatusasofJune30\": \"object\",\n",
    "            \"BaseSalary\": \"float64\",\n",
    "            \"PayBasis\": \"object\",\n",
    "            \"RegularHours\": \"float64\",\n",
    "            \"RegularGrossPaid\": \"float64\",\n",
    "            \"OTHours\": \"float64\",\n",
    "            \"TotalOTPaid\": \"float64\",\n",
    "            \"TotalOtherPay\": \"float64\"\n",
    "        }\n",
    "\n",
    "        # Load the data\n",
    "        NYCpayroll_df = dd.read_csv(r\"Dataset\\Raw_payroll_Data\\nycpayroll_2020.csv\", dtype=dtypes)\n",
    "\n",
    "        # Drop duplicates\n",
    "        NYCpayroll_df = NYCpayroll_df.drop_duplicates()\n",
    "\n",
    "        # Convert date columns to datetime\n",
    "        NYCpayroll_df['AgencyStartDate'] = dd.to_datetime(NYCpayroll_df['AgencyStartDate'], errors='coerce')\n",
    "\n",
    "        # Rename columns\n",
    "        NYCpayroll_df = NYCpayroll_df.rename(columns={'AgencyCode': 'AgencyID'})\n",
    "\n",
    "        # Fill missing values\n",
    "        NYCpayroll_df = NYCpayroll_df.fillna({\"FiscalYear\": 0,\n",
    "                                                \"PayrollNumber\": 0,\n",
    "                                                \"AgencyID\": 0,\n",
    "                                                \"AgencyName\": \"Unknown\",\n",
    "                                                \"EmployeeID\": 0,\n",
    "                                                \"LastName\": \"Unknown\",\n",
    "                                                \"FirstName\": \"Unknown\",\n",
    "                                                \"AgencyStartDate\": pd.Timestamp('1900-01-01'),  # Default date for missing values\n",
    "                                                \"WorkLocationBorough\": \"Unknown\",\n",
    "                                                \"TitleCode\": 0,\n",
    "                                                \"TitleDescription\": \"Unknown\",\n",
    "                                                \"LeaveStatusasofJune30\": \"Unknown\",\n",
    "                                                \"BaseSalary\": NYCpayroll_df[\"BaseSalary\"].mean(),\n",
    "                                                \"PayBasis\": \"Unknown\",\n",
    "                                                \"RegularHours\": NYCpayroll_df[\"RegularHours\"].mean(),\n",
    "                                                \"RegularGrossPaid\": NYCpayroll_df[\"RegularGrossPaid\"].mean(),\n",
    "                                                \"OTHours\": NYCpayroll_df[\"OTHours\"].mean(),\n",
    "                                                \"TotalOTPaid\": NYCpayroll_df[\"TotalOTPaid\"].mean(),\n",
    "                                                \"TotalOtherPay\": NYCpayroll_df[\"TotalOtherPay\"].mean()\n",
    "                                            })\n",
    "\n",
    "        logging.info(\"Data transformation completed successfully.\")\n",
    "\n",
    "        # converting dask to pandas to enable loading to snowflakes\n",
    "        NYCpayroll_df = NYCpayroll_df.compute() \n",
    "        # Save the transformed data back to disk\n",
    "        NYCpayroll_df.to_csv(output_file_path, index=False)\n",
    "        logging.info(f\"Transformed data saved to {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data transformation: {e}\")\n",
    "        raise\n",
    "\n",
    "    return NYCpayroll_df\n",
    "\n",
    "    # Trigger the transformation\n",
    "transform_data(\n",
    "    r\"Dataset/Raw_payroll_Data/nycpayroll_2020.csv\", \n",
    "    r\"Dataset/Cleaned_payroll_Data\\nycpayroll_2020.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform.py\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/transformation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform_data(input_file_path):\n",
    "    \"\"\"\n",
    "    Transforms raw NYC payroll data.\n",
    "\n",
    "    Parameters:\n",
    "        input_file_path (str): Path to the raw input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dask.dataframe.DataFrame: Transformed Dask DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting data transformation.\")\n",
    "\n",
    "        # Define data types for columns\n",
    "        dtypes = {\n",
    "            \"FiscalYear\": \"int64\",\n",
    "            \"PayrollNumber\": \"int64\",\n",
    "            \"AgencyID\": \"int64\",\n",
    "            \"AgencyName\": \"object\",\n",
    "            \"EmployeeID\": \"int64\",\n",
    "            \"LastName\": \"object\",\n",
    "            \"FirstName\": \"object\",\n",
    "            \"AgencyStartDate\": \"object\",\n",
    "            \"WorkLocationBorough\": \"object\",\n",
    "            \"TitleCode\": \"int64\",\n",
    "            \"TitleDescription\": \"object\",\n",
    "            \"LeaveStatusasofJune30\": \"object\",\n",
    "            \"BaseSalary\": \"float64\",\n",
    "            \"PayBasis\": \"object\",\n",
    "            \"RegularHours\": \"float64\",\n",
    "            \"RegularGrossPaid\": \"float64\",\n",
    "            \"OTHours\": \"float64\",\n",
    "            \"TotalOTPaid\": \"float64\",\n",
    "            \"TotalOtherPay\": \"float64\"\n",
    "        }\n",
    "\n",
    "        # Load data into a Dask DataFrame\n",
    "        logging.info(f\"Loading data from {\"Raw_payroll_Data\\nycpayroll_2020.csv\"}\")\n",
    "        NYCpayroll_df = dd.read_csv(r\"Raw_payroll_Data\\nycpayroll_2020.csv\", dtype=dtypes)\n",
    "        df = dd.read_csv(input_file_path, dtype=dtypes, assume_missing=True)\n",
    "\n",
    "        # Drop duplicates\n",
    "        df = df.drop_duplicates()\n",
    "        logging.info(\"Dropped duplicate rows.\")\n",
    "\n",
    "        # Convert 'AgencyStartDate' to datetime\n",
    "        df['AgencyStartDate'] = dd.to_datetime(df['AgencyStartDate'], errors='coerce')\n",
    "        logging.info(\"Converted 'AgencyStartDate' to datetime.\")\n",
    "\n",
    "        # Rename columns if necessary\n",
    "        # Example: df = df.rename(columns={'OldColumnName': 'NewColumnName'})\n",
    "        # logging.info(\"Renamed columns.\")\n",
    "\n",
    "        # Calculate mean values for numerical columns to fill missing data\n",
    "        base_salary_mean = df['BaseSalary'].mean().compute()\n",
    "        regular_hours_mean = df['RegularHours'].mean().compute()\n",
    "        regular_gross_paid_mean = df['RegularGrossPaid'].mean().compute()\n",
    "        ot_hours_mean = df['OTHours'].mean().compute()\n",
    "        total_ot_paid_mean = df['TotalOTPaid'].mean().compute()\n",
    "        total_other_pay_mean = df['TotalOtherPay'].mean().compute()\n",
    "\n",
    "        # Fill missing values\n",
    "        df = df.fillna({\n",
    "            \"FiscalYear\": 0,\n",
    "            \"PayrollNumber\": 0,\n",
    "            \"AgencyID\": 0,\n",
    "            \"AgencyName\": \"Unknown\",\n",
    "            \"EmployeeID\": 0,\n",
    "            \"LastName\": \"Unknown\",\n",
    "            \"FirstName\": \"Unknown\",\n",
    "            \"AgencyStartDate\": pd.Timestamp('1900-01-01'),\n",
    "            \"WorkLocationBorough\": \"Unknown\",\n",
    "            \"TitleCode\": 0,\n",
    "            \"TitleDescription\": \"Unknown\",\n",
    "            \"LeaveStatusasofJune30\": \"Unknown\",\n",
    "            \"BaseSalary\": base_salary_mean,\n",
    "            \"PayBasis\": \"Unknown\",\n",
    "            \"RegularHours\": regular_hours_mean,\n",
    "            \"RegularGrossPaid\": regular_gross_paid_mean,\n",
    "            \"OTHours\": ot_hours_mean,\n",
    "            \"TotalOTPaid\": total_ot_paid_mean,\n",
    "            \"TotalOtherPay\": total_other_pay_mean\n",
    "        })\n",
    "        logging.info(\"Filled missing values.\")\n",
    "\n",
    "        logging.info(\"Data transformation completed successfully.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during transformation: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID',\n",
       "       'LastName', 'FirstName', 'AgencyStartDate', 'WorkLocationBorough',\n",
       "       'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30', 'BaseSalary',\n",
       "       'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours',\n",
       "       'TotalOTPaid', 'TotalOtherPay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYCpayroll_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of Dask DataFrame Structure:\n",
       "              FiscalYear PayrollNumber AgencyID AgencyName EmployeeID LastName FirstName AgencyStartDate WorkLocationBorough TitleCode TitleDescription LeaveStatusasofJune30 BaseSalary PayBasis RegularHours RegularGrossPaid  OTHours TotalOTPaid TotalOtherPay\n",
       "npartitions=1                                                                                                                                                                                                                                                     \n",
       "                   int64         int64    int64     string      int64   string    string          string              string     int64           string                string    float64   string      float64          float64  float64     float64       float64\n",
       "                     ...           ...      ...        ...        ...      ...       ...             ...                 ...       ...              ...                   ...        ...      ...          ...              ...      ...         ...           ...\n",
       "Dask Name: read_csv, 1 expression\n",
       "Expr=ReadCSV(3cd450b)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYCpayroll_df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 1: Generate minimal dim_date based on FiscalYear\n",
    "def generate_dim_date(fiscal_years):\n",
    "    dim_date = pd.DataFrame({\n",
    "        'DateKey': [int(f\"{year}0101\") for year in fiscal_years],\n",
    "        'Date': [pd.Timestamp(f\"{year}-01-01\") for year in fiscal_years],\n",
    "        'Year': fiscal_years,\n",
    "        'Month': [1] * len(fiscal_years),\n",
    "        'Day': [1] * len(fiscal_years),\n",
    "        'Quarter': [1] * len(fiscal_years)\n",
    "    })\n",
    "    return dim_date\n",
    "\n",
    "# Extract distinct fiscal years\n",
    "fiscal_years = NYCpayroll_df['FiscalYear'].drop_duplicates().compute().tolist()\n",
    "\n",
    "# Generate dim_date DataFrame\n",
    "dim_date_df = generate_dim_date(fiscal_years)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateKey</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200101</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19980101</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DateKey       Date  Year  Month  Day  Quarter\n",
       "0  20200101 2020-01-01  2020      1    1        1\n",
       "1  19980101 1998-01-01  1998      1    1        1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dim_date_df to CSV (for loading into Snowflake later)\n",
    "dim_date_output_path = os.path.join(\"<path_to_save_dim_date>\", \"dim_date.csv\")\n",
    "dim_date_df.to_csv(dim_date_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FiscalYear                         int64\n",
      "PayrollNumber                      int64\n",
      "AgencyID                           int64\n",
      "AgencyName               string[pyarrow]\n",
      "EmployeeID                         int64\n",
      "LastName                 string[pyarrow]\n",
      "FirstName                string[pyarrow]\n",
      "AgencyStartDate          string[pyarrow]\n",
      "WorkLocationBorough      string[pyarrow]\n",
      "TitleCode                          int64\n",
      "TitleDescription         string[pyarrow]\n",
      "LeaveStatusasofJune30    string[pyarrow]\n",
      "BaseSalary                       float64\n",
      "PayBasis                 string[pyarrow]\n",
      "RegularHours                     float64\n",
      "RegularGrossPaid                 float64\n",
      "OTHours                          float64\n",
      "TotalOTPaid                      float64\n",
      "TotalOtherPay                    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(NYCpayroll_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr._collection.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(FiscalYear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# File paths (placeholders)\n",
    "input_file_path = \"<path_to_input_file>\"\n",
    "output_file_path = \"<path_to_output_file>\"\n",
    "\n",
    "# Load the CSV file as a Dask DataFrame\n",
    "NYCpayroll_df = dd.read_csv(input_file_path)\n",
    "\n",
    "# Transformation 1: Generate minimal dim_date based on FiscalYear\n",
    "def generate_dim_date(fiscal_years):\n",
    "    dim_date = pd.DataFrame({\n",
    "        'DateKey': [int(f\"{year}0101\") for year in fiscal_years],\n",
    "        'Date': [pd.Timestamp(f\"{year}-01-01\") for year in fiscal_years],\n",
    "        'Year': fiscal_years,\n",
    "        'Month': [1] * len(fiscal_years),\n",
    "        'Day': [1] * len(fiscal_years),\n",
    "        'Quarter': [1] * len(fiscal_years)\n",
    "    })\n",
    "    return dim_date\n",
    "\n",
    "# Extract distinct fiscal years\n",
    "fiscal_years = NYCpayroll_df['FiscalYear'].drop_duplicates().compute().tolist()\n",
    "\n",
    "# Generate dim_date DataFrame\n",
    "dim_date_df = generate_dim_date(fiscal_years)\n",
    "\n",
    "# Save dim_date_df to CSV (for loading into Snowflake later)\n",
    "dim_date_output_path = os.path.join(\"<path_to_save_dim_date>\", \"dim_date.csv\")\n",
    "dim_date_df.to_csv(dim_date_output_path, index=False)\n",
    "\n",
    "# Transformation 2: Add PayBasisID and PayBasisDescription columns\n",
    "def map_paybasis(df):\n",
    "    # Generate unique IDs for PayBasisDescription\n",
    "    df['PayBasisID'] = df['PayBasisDescription'].factorize()[0] + 1\n",
    "    return df\n",
    "\n",
    "# Apply the mapping function\n",
    "NYCpayroll_df = NYCpayroll_df.map_partitions(map_paybasis)\n",
    "\n",
    "# Other transformations (e.g., dropping duplicates, handling missing values, renaming columns)\n",
    "NYCpayroll_df = NYCpayroll_df.drop_duplicates()\n",
    "NYCpayroll_df['AgencyStartDate'] = dd.to_datetime(NYCpayroll_df['AgencyStartDate'], errors='coerce')\n",
    "NYCpayroll_df = NYCpayroll_df.rename(columns={'AgencyCode': 'AgencyID'})\n",
    "\n",
    "# Save the transformed data to CSV\n",
    "NYCpayroll_df.to_csv(output_file_path, index=False, single_file=True)\n",
    "\n",
    "print(\"Transformation complete and files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(NYCpayroll_df):\n",
    "    try:\n",
    "            \n",
    "        dtypes = {\n",
    "        \"FiscalYear\" : \"int64\",\n",
    "        \"PayrollNumber\" : \"int64\",\n",
    "        \"AgencyID\" : \"int64\",\n",
    "        \"AgencyName\" : \"object\",\n",
    "        \"EmployeeID\" : \"int64\",\n",
    "        \"LastName\" : \"object\",\n",
    "        \"FirstName\"  : \"object\",\n",
    "        \"AgencyStartDate\" : \"object\",\n",
    "        \"WorkLocationBorough\" : \"object\", \n",
    "        \"TitleCode\" : \"int64\",\n",
    "        \"TitleDescription\" : \"object\",\n",
    "        \"LeaveStatusasofJune30\" : \"object\",\n",
    "        \"BaseSalary\" : \"float64\",\n",
    "        \"PayBasis\" : \"object\" ,\n",
    "        \"RegularHours\" : \"float64\",\n",
    "        \"RegularGrossPaid\" : \"float64\",\n",
    "        \"OTHours\" : \"float64\",\n",
    "        \"TotalOTPaid\" : \"float64\",\n",
    "        \"TotalOtherPay\" : \"float64\"\n",
    "    }\n",
    "        NYCpayroll_df = dd.read_csv(r\"Raw_payroll_Data\\nycpayroll_2020.csv\", dtype=dtypes)\n",
    "\n",
    "        # Drop duplicates\n",
    "\n",
    "        NYCpayroll_df = NYCpayroll_df.drop_duplicates()\n",
    "\n",
    "        # Convert date columns to datetime\n",
    "        NYCpayroll_df['AgencyStartDate'] = dd.to_datetime(NYCpayroll_df['AgencyStartDate'], errors='coerce', inplace=True)\n",
    "\n",
    "        # Rename columns\n",
    "        NYCpayroll_df = NYCpayroll_df.rename(columns={'AgencyCode': 'AgencyID'}, inplace=True)\n",
    "\n",
    "\n",
    "        # Fill missing values\n",
    "        NYCpayroll_df = NYCpayroll_df.fillna ({\"FiscalYear\" : 0000,\n",
    "                                                \"PayrollNumber\" : 00,\n",
    "                                                \"AgencyID\" : 0000,\n",
    "                                                \"AgencyName\" : \"Unknown\",\n",
    "                                                \"EmployeeID\" : 000000,\n",
    "                                                \"LastName\" : \"Unknown\",\n",
    "                                                \"FirstName\"  : \"Unknown\",\n",
    "                                                \"AgencyStartDate\" : pd.Timestamp('0000/00/00'),\n",
    "                                                \"WorkLocationBorough\" : \"Unknown\", \n",
    "                                                \"TitleCode\" : 00000,\n",
    "                                                \"TitleDescription\" : \"Unknown\",\n",
    "                                                \"LeaveStatusasofJune30\" : \"Unknown\",\n",
    "                                                \"BaseSalary\" : NYCpayroll_df[\"BaseSalary\"].mean(),\n",
    "                                                \"PayBasis\" : \"Unknown\",\n",
    "                                                \"RegularHours\" : NYCpayroll_df[\"BaseSalary\"].mean(),\n",
    "                                                \"RegularGrossPaid\" : NYCpayroll_df[\"BaseSalary\"].mean(),\n",
    "                                                \"OTHours\" : NYCpayroll_df[\"BaseSalary\"].mean(),\n",
    "                                                \"TotalOTPaid\" : NYCpayroll_df[\"BaseSalary\"].mean(),\n",
    "                                                \"TotalOtherPay\" : NYCpayroll_df[\"BaseSalary\"].mean()\n",
    "                                            },   inplace=True)\n",
    "\n",
    "        logging.info(\"Data transformation completed successfully.\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data transformation: {e}\")\n",
    "        raise\n",
    "\n",
    "    return NYCpayroll_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s  %(levelname)s  %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"transformation.log\"),  # Logs to a file named transformation.log\n",
    "        logging.StreamHandler()  # Also prints logs to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform_data(input_file_path, output_file_path):\n",
    "    try:\n",
    "        logging.info(\"Starting data transformation.\")\n",
    "\n",
    "        # Define data types for columns\n",
    "        dtypes = {\n",
    "            \"FiscalYear\": \"int64\",\n",
    "            \"PayrollNumber\": \"int64\",\n",
    "            \"AgencyID\": \"int64\",\n",
    "            \"AgencyName\": \"object\",\n",
    "            \"EmployeeID\": \"int64\",\n",
    "            \"LastName\": \"object\",\n",
    "            \"FirstName\": \"object\",\n",
    "            \"AgencyStartDate\": \"object\",\n",
    "            \"WorkLocationBorough\": \"object\",\n",
    "            \"TitleCode\": \"int64\",\n",
    "            \"TitleDescription\": \"object\",\n",
    "            \"LeaveStatusasofJune30\": \"object\",\n",
    "            \"BaseSalary\": \"float64\",\n",
    "            \"PayBasis\": \"object\",\n",
    "            \"RegularHours\": \"float64\",\n",
    "            \"RegularGrossPaid\": \"float64\",\n",
    "            \"OTHours\": \"float64\",\n",
    "            \"TotalOTPaid\": \"float64\",\n",
    "            \"TotalOtherPay\": \"float64\"\n",
    "        }\n",
    "\n",
    "        # Load the data\n",
    "        NYCpayroll_df = dd.read_csv(input_file_path, dtype=dtypes)\n",
    "\n",
    "        # Drop duplicates\n",
    "        NYCpayroll_df = NYCpayroll_df.drop_duplicates()\n",
    "\n",
    "        # Convert date columns to datetime\n",
    "        NYCpayroll_df['AgencyStartDate'] = dd.to_datetime(NYCpayroll_df['AgencyStartDate'], errors='coerce')\n",
    "\n",
    "        # Rename columns\n",
    "        NYCpayroll_df = NYCpayroll_df.rename(columns={'AgencyCode': 'AgencyID'})\n",
    "\n",
    "        # Fill missing values\n",
    "        NYCpayroll_df = NYCpayroll_df.fillna({\n",
    "            \"FiscalYear\": 0,\n",
    "            \"PayrollNumber\": 0,\n",
    "            \"AgencyID\": 0,\n",
    "            \"AgencyName\": \"Unknown\",\n",
    "            \"EmployeeID\": 0,\n",
    "            \"LastName\": \"Unknown\",\n",
    "            \"FirstName\": \"Unknown\",\n",
    "            \"AgencyStartDate\": pd.Timestamp('1900-01-01'),  # Default date for missing values\n",
    "            \"WorkLocationBorough\": \"Unknown\",\n",
    "            \"TitleCode\": 0,\n",
    "            \"TitleDescription\": \"Unknown\",\n",
    "            \"LeaveStatusasofJune30\": \"Unknown\",\n",
    "            \"BaseSalary\": NYCpayroll_df[\"BaseSalary\"].mean(),\n",
    "            \"PayBasis\": \"Unknown\",\n",
    "            \"RegularHours\": NYCpayroll_df[\"RegularHours\"].mean(),\n",
    "            \"RegularGrossPaid\": NYCpayroll_df[\"RegularGrossPaid\"].mean(),\n",
    "            \"OTHours\": NYCpayroll_df[\"OTHours\"].mean(),\n",
    "            \"TotalOTPaid\": NYCpayroll_df[\"TotalOTPaid\"].mean(),\n",
    "            \"TotalOtherPay\": NYCpayroll_df[\"TotalOtherPay\"].mean()\n",
    "        })\n",
    "\n",
    "        logging.info(\"Data transformation completed successfully.\")\n",
    "\n",
    "        # Save the transformed data back to disk\n",
    "        NYCpayroll_df.to_csv(output_file_path, index=False, single_file=True)\n",
    "        logging.info(f\"Transformed data saved to {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data transformation: {e}\")\n",
    "        raise\n",
    "\n",
    "    return NYCpayroll_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Placeholder: Replace with actual paths\n",
    "    input_file_path = r\"Raw_payroll_Data/nycpayroll_2020.csv\"\n",
    "    output_file_path = r\"Transformed_payroll_Data/nycpayroll_2020_transformed.csv\"\n",
    "\n",
    "    # Trigger the transformation\n",
    "    transform_data(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''logging.basicConfig(\n",
    "    filename='ETL_pipeline.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s\\n\\n'  # Adjust the format as needed\n",
    ")\n",
    "'''\n",
    "#format = ('%(asctime)s' \n",
    "    #         '%(levelname)s'  \n",
    "    #         '%(message)s\\n\\n'#\n",
    "   # ),\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"running_pipeline.log\",\n",
    "    level=logging.INFO,\n",
    "    format = '%(asctime)s  %(levelname)s  %(message)s\\n\\n',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"running_pipeline.log\"),  # Logs to a file named transformation.log\n",
    "        logging.StreamHandler()  # Also prints logs to the console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FiscalYear                         int64\n",
      "PayrollNumber                      int64\n",
      "AgencyID                           int64\n",
      "AgencyName               string[pyarrow]\n",
      "EmployeeID                         int64\n",
      "LastName                 string[pyarrow]\n",
      "FirstName                string[pyarrow]\n",
      "AgencyStartDate          string[pyarrow]\n",
      "WorkLocationBorough      string[pyarrow]\n",
      "TitleCode                          int64\n",
      "TitleDescription         string[pyarrow]\n",
      "LeaveStatusasofJune30    string[pyarrow]\n",
      "BaseSalary                       float64\n",
      "PayBasis                 string[pyarrow]\n",
      "RegularHours                     float64\n",
      "RegularGrossPaid                 float64\n",
      "OTHours                          float64\n",
      "TotalOTPaid                      float64\n",
      "TotalOtherPay                    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(NYCpayroll_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID',\n",
      "       'LastName', 'FirstName', 'AgencyStartDate', 'WorkLocationBorough',\n",
      "       'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30', 'BaseSalary',\n",
      "       'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours',\n",
      "       'TotalOTPaid', 'TotalOtherPay'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(NYCpayroll_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Tables:\n",
    "\n",
    "# Dim_Date_Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim_Employee_Table\n",
    "\n",
    "Employee = NYCpayroll_df[['EmployeeID', 'LastName', 'FirstName', 'AgencyStartDate', 'AgencyID', 'WorkLocationBorough','TitleCode',\\\n",
    "                          'TitleDescription', 'LeaveStatusasofJune30']].copy().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>AgencyStartDate</th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>WorkLocationBorough</th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>TitleDescription</th>\n",
       "      <th>LeaveStatusasofJune30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>GEAGER</td>\n",
       "      <td>VERONICA</td>\n",
       "      <td>9/12/2016</td>\n",
       "      <td>2120</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149612</td>\n",
       "      <td>ROTTA</td>\n",
       "      <td>JONATHAN</td>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>2120</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206583</td>\n",
       "      <td>WILSON II</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>4/30/2018</td>\n",
       "      <td>2120</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199874</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>MORIAH</td>\n",
       "      <td>3/18/2019</td>\n",
       "      <td>2120</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58036</td>\n",
       "      <td>KRAWCZYK</td>\n",
       "      <td>AMANDA</td>\n",
       "      <td>5/15/2017</td>\n",
       "      <td>2120</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeID    LastName FirstName AgencyStartDate  AgencyID  \\\n",
       "0       10001      GEAGER  VERONICA       9/12/2016      2120   \n",
       "1      149612       ROTTA  JONATHAN       9/16/2013      2120   \n",
       "2      206583   WILSON II    ROBERT       4/30/2018      2120   \n",
       "3      199874  WASHINGTON    MORIAH       3/18/2019      2120   \n",
       "4       58036    KRAWCZYK    AMANDA       5/15/2017      2120   \n",
       "\n",
       "  WorkLocationBorough  TitleCode                TitleDescription  \\\n",
       "0            BROOKLYN      40447  EMERGENCY PREPAREDNESS MANAGER   \n",
       "1            BROOKLYN      40447  EMERGENCY PREPAREDNESS MANAGER   \n",
       "2            BROOKLYN      40447  EMERGENCY PREPAREDNESS MANAGER   \n",
       "3            BROOKLYN      40447  EMERGENCY PREPAREDNESS MANAGER   \n",
       "4            BROOKLYN      40447  EMERGENCY PREPAREDNESS MANAGER   \n",
       "\n",
       "  LeaveStatusasofJune30  \n",
       "0                ACTIVE  \n",
       "1                ACTIVE  \n",
       "2                ACTIVE  \n",
       "3                ACTIVE  \n",
       "4                ACTIVE  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Employee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim_Agency_Table\n",
    "\n",
    "Agency = NYCpayroll_df[['AgencyID', 'AgencyName']].copy().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>AgencyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2122</td>\n",
       "      <td>OFFICE OF MANAGEMENT &amp; BUDGET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgencyID                      AgencyName\n",
       "0      2120  OFFICE OF EMERGENCY MANAGEMENT\n",
       "1      2122   OFFICE OF MANAGEMENT & BUDGET"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim_PayBasis_Table\n",
    "\n",
    "PayBasis = NYCpayroll_df[['PayBasis']]\n",
    "PayBasis['PayBasisID'] = range(1, len(PayBasis) + 1)\n",
    "PayBasis = PayBasis[['PayBasisID', 'PayBasis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 2 entries, PayBasisID to PayBasis\n",
      "dtypes: int64(1), string(1)"
     ]
    }
   ],
   "source": [
    "PayBasis.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC_Payroll_Fact _Table\n",
    "\n",
    "Fact = [['FiscalYear', 'FactID' 'AgencyID', 'PayBasisID', 'EmployeeID', 'PayrollNumber', 'BaseSalary','RegularHours', 'RegularGrossPaid',\\\n",
    "          'OTHours', 'TotalOTPaid', 'TotalOtherPay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconn_params\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conn_params' is not defined"
     ]
    }
   ],
   "source": [
    "print(conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oluwa\\anaconda3\\envs\\mygotoenv\\lib\\site-packages\\snowflake\\sqlalchemy\\base.py:1068: SAWarning: The GenericFunction 'flatten' is already registered and is going to be overridden.\n",
      "  functions.register_function(\"flatten\", flatten)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dim_date_df' from 'load' (c:\\Users\\oluwa\\OneDrive\\Documents\\Data Engineering Training - 10Alytics\\Project - Capstone\\NYCPayroll_ETL\\load.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dim_date_df, transformed_nycpayroll_df\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dim_date_df' from 'load' (c:\\Users\\oluwa\\OneDrive\\Documents\\Data Engineering Training - 10Alytics\\Project - Capstone\\NYCPayroll_ETL\\load.py)"
     ]
    }
   ],
   "source": [
    "from load import dim_date_df, transformed_nycpayroll_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dim_date_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdim_date_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dim_date_df' is not defined"
     ]
    }
   ],
   "source": [
    "dim_date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data_to_snowflake(transformed_file_path, dim_date_file_path):\n",
    "\n",
    "    try:\n",
    "\n",
    "         # Initialize Dask client with 1 worker\n",
    "        client = Client(n_workers=1)\n",
    "        logging.info(\"Dask client initialized with 1 worker.\")\n",
    "        # Get Snowflake engine\n",
    "        engine = get_snowflake_engine()\n",
    "\n",
    "\n",
    "         # Define table structures\n",
    "       \n",
    "        tables = [\n",
    "            {\n",
    "                \"name\": \"Dim_Employee\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"EmployeeID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"LastName\", \"type\": \"VARCHAR(50)\"},\n",
    "                    {\"name\": \"FirstName\", \"type\": \"VARCHAR(50)\"},\n",
    "                    {\"name\": \"AgencyStartDate\", \"type\": \"DATE\"},\n",
    "                    {\"name\": \"TitleCode\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"TitleDescription\", \"type\": \"TEXT\"},\n",
    "                    {\"name\": \"WorkLocationBorough\", \"type\": \"VARCHAR(255)\"},\n",
    "                    {\"name\": \"LeaveStatusasofJune30\", \"type\": \"VARCHAR(50)\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Dim_Agency\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"AgencyID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"AgencyName\", \"type\": \"VARCHAR(255)\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Dim_PayBasis\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"PayBasisID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"PayBasis\", \"type\": \"VARCHAR(255)\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Dim_Date\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"DateID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Date\", \"type\": \"DATE\"},\n",
    "                    {\"name\": \"Year\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Month\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Day\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Quarter\", \"type\": \"INT\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FactPayroll\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"FactID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"EmployeeID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"AgencyID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"DateID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"PayBasisID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"FiscalYear\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"PayrollNumber\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"BaseSalary\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"RegularHours\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"OTHours\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"RegularGrossPaid\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"TotalOTPaid\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"TotalOtherPay\", \"type\": \"FLOAT\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Step 1: Load the main transformed CSV file\n",
    "        transformed_nycpayroll_df = pd.read_csv(\"Dataset/Cleaned_payroll_Data/nycpayroll_2020.csv\")\n",
    "        logging.info(\"Transformed NycPayroll CSV file loaded successfully.\")\n",
    "\n",
    "        # Step 2: Load data into Snowflake\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "\n",
    "            # Step 3: Truncate the tables before loading new data\n",
    "            for table in tables:\n",
    "                table_name = table[\"name\"]\n",
    "                columns = table[\"columns\"]\n",
    "                column_defs = ', '.join([f'\"{col[\"name\"]}\" {col[\"type\"]}' for col in columns])\n",
    "                create_table_sql = f'CREATE TABLE IF NOT EXISTS \"{table_name}\" ({column_defs});'\n",
    "                conn.execute(text(create_table_sql))\n",
    "                logging.info(f'Table \"{table_name}\" created or already exists.')\n",
    "\n",
    "            # Step 2: Truncate the tables before loading new data\n",
    "            for table in tables:\n",
    "                truncate_table_sql = f'TRUNCATE TABLE \"{table[\"name\"]}\";'\n",
    "                conn.execute(text(truncate_table_sql))\n",
    "                logging.info(f'Truncated \"{table[\"name\"]}\" successfully.')\n",
    "\n",
    "            # Dim_Employee\n",
    "            Dim_Employee_df = transformed_nycpayroll_df[[\n",
    "                \"EmployeeID\", \"LastName\", \"FirstName\", \"AgencyStartDate\", \n",
    "                \"TitleCode\", \"TitleDescription\", \"WorkLocationBorough\", \n",
    "                \"LeaveStatusasofJune30\"\n",
    "            ]]\n",
    "            Dim_Employee_df.to_sql(\"Dim_Employee\", con=conn, if_exists=\"replace\", index=False)\n",
    "            logging.info('Dim_Employee data loaded successfully!!')\n",
    "\n",
    "            # Dim_Agency\n",
    "            Dim_Agency_df = transformed_nycpayroll_df[[\"AgencyID\", \"AgencyName\"]]\n",
    "            Dim_Agency_df.to_sql(\"Dim_Agency\", con=conn, if_exists=\"replace\", index=False)\n",
    "            logging.info('Dim_Agency data loaded successfully!!')\n",
    "\n",
    "            # Dim_PayBasis\n",
    "            dim_paybasis_df = transformed_nycpayroll_df[[\"PayBasisID\", \"PayBasis\"]]\n",
    "            dim_paybasis_df.to_sql(\"Dim_PayBasis\", con=conn, if_exists=\"replace\", index=False)\n",
    "            logging.info('Dim_PayBasis data loaded successfully!!')\n",
    "\n",
    "            # Dim_Date (from dim_date.csv)\n",
    "            dim_date_df = pd.read_csv(\"Dataset/Cleaned_payroll_Data/dim_date.csv\")\n",
    "            dim_date_df.to_sql(\"Dim_Date\", con=conn, if_exists=\"replace\", index=False)\n",
    "            logging.info('Dim_Date data loaded successfully.')\n",
    "\n",
    "            # FactPayroll\n",
    "            # Merge with dim_date_df to include DateID\n",
    "            factpayroll_df = transformed_nycpayroll_df.merge(dim_date_df, left_on=\"FiscalYear\", right_on=\"Year\", how=\"inner\") \\\n",
    "                                                        [[\"EmployeeID\", \"AgencyID\", \"DateID\", \"PayBasisID\", \"FiscalYear\", \\\n",
    "                                                        \"PayrollNumber\", \"BaseSalary\", \"RegularHours\", \"OTHours\", \\\n",
    "                                                        \"RegularGrossPaid\", \"TotalOTPaid\", \"TotalOtherPay\"]]\n",
    "\n",
    "            # Creating the FactID after the merging\n",
    "            factpayroll_df[\"FactID\"] = range(1, len(factpayroll_df) + 1)\n",
    "\n",
    "            # Reorder columns to have FactID first\n",
    "            factpayroll_df = factpayroll_df[[\n",
    "                \"FactID\", \"EmployeeID\", \"AgencyID\", \"DateID\", \"PayBasisID\", \"FiscalYear\", \\\n",
    "                \"PayrollNumber\", \"BaseSalary\", \"RegularHours\", \"OTHours\", \"RegularGrossPaid\", \\\n",
    "                \"TotalOTPaid\", \"TotalOtherPay\"\n",
    "            ]]\n",
    "\n",
    "            factpayroll_df.to_sql(\"FactPayroll\", con=conn, if_exists=\"replace\", index=False)\n",
    "            logging.info('FactPayroll data loaded successfully!!')\n",
    "\n",
    "            # Commit the changes\n",
    "            logging.info(\"All data loaded successfully!!\")\n",
    "\n",
    "        # Close the Dask client\n",
    "        client.close()\n",
    "        logging.info(\"Dask client closed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data into Snowflake: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import snowflake.sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pipeline_log import logging\n",
    "from dask.distributed import Client\n",
    "from sqlalchemy import text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the Snowflake connection parameters\n",
    "def get_snowflake_engine():\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            \"snowflake://{user}:{password}@{account_identifier}/{database}/{schema}?warehouse={warehouse}\".format(\n",
    "                user=os.getenv(\"sn_user\"),\n",
    "                password=os.getenv(\"sn_pword\"),\n",
    "                account_identifier=os.getenv(\"sn_Acct_Id\"),\n",
    "                database=os.getenv(\"sn_DB\"),\n",
    "                schema=os.getenv(\"sn_schema\"),\n",
    "                warehouse=os.getenv(\"sn_DWH\")\n",
    "            )\n",
    "        )\n",
    "        logging.info(\"Connected to Snowflake successfully.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to Snowflake: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables = [\n",
    "            {\n",
    "                \"name\": \"Dim_Employee\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"EmployeeID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"LastName\", \"type\": \"VARCHAR(50)\"},\n",
    "                    {\"name\": \"FirstName\", \"type\": \"VARCHAR(50)\"},\n",
    "                    {\"name\": \"AgencyStartDate\", \"type\": \"DATE\"},\n",
    "                    {\"name\": \"TitleCode\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"TitleDescription\", \"type\": \"TEXT\"},\n",
    "                    {\"name\": \"WorkLocationBorough\", \"type\": \"VARCHAR(255)\"},\n",
    "                    {\"name\": \"LeaveStatusasofJune30\", \"type\": \"VARCHAR(50)\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Dim_Agency\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"AgencyID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"AgencyName\", \"type\": \"VARCHAR(255)\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Dim_PayBasis\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"PayBasisID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"PayBasis\", \"type\": \"VARCHAR(255)\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Dim_Date\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"DateID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Date\", \"type\": \"DATE\"},\n",
    "                    {\"name\": \"Year\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Month\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Day\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"Quarter\", \"type\": \"INT\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FactPayroll\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"FactID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"EmployeeID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"AgencyID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"DateID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"PayBasisID\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"FiscalYear\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"PayrollNumber\", \"type\": \"INT\"},\n",
    "                    {\"name\": \"BaseSalary\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"RegularHours\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"OTHours\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"RegularGrossPaid\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"TotalOTPaid\", \"type\": \"FLOAT\"},\n",
    "                    {\"name\": \"TotalOtherPay\", \"type\": \"FLOAT\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "                table_name = table[\"name\"]\n",
    "                columns = table[\"columns\"]\n",
    "                column_defs = ', '.join([f'\"{col[\"name\"]}\" {col[\"type\"]}' for col in columns])\n",
    "                create_table_sql = f'CREATE TABLE IF NOT EXISTS \"{table_name}\" ({column_defs});'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"FactID\" INT, \"EmployeeID\" INT, \"AgencyID\" INT, \"DateID\" INT, \"PayBasisID\" INT, \"FiscalYear\" INT, \"PayrollNumber\" INT, \"BaseSalary\" FLOAT, \"RegularHours\" FLOAT, \"OTHours\" FLOAT, \"RegularGrossPaid\" FLOAT, \"TotalOTPaid\" FLOAT, \"TotalOtherPay\" FLOAT\n"
     ]
    }
   ],
   "source": [
    "print(column_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 07:32:55,203 - INFO - Transformed NycPayroll CSV file loaded successfully.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Step 1: Load the main transformed CSV file\n",
    "transformed_nycpayroll_df = pd.read_csv(\"Dataset/Cleaned_payroll_Data/nycpayroll_2020.csv\")\n",
    "logging.info(\"Transformed NycPayroll CSV file loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>AgencyStartDate</th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>TitleDescription</th>\n",
       "      <th>WorkLocationBorough</th>\n",
       "      <th>LeaveStatusasofJune30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>GEAGER</td>\n",
       "      <td>VERONICA</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149612</td>\n",
       "      <td>ROTTA</td>\n",
       "      <td>JONATHAN</td>\n",
       "      <td>2013-09-16</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206583</td>\n",
       "      <td>WILSON II</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199874</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>MORIAH</td>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58036</td>\n",
       "      <td>KRAWCZYK</td>\n",
       "      <td>AMANDA</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>40447</td>\n",
       "      <td>EMERGENCY PREPAREDNESS MANAGER</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeID    LastName FirstName AgencyStartDate  TitleCode  \\\n",
       "0       10001      GEAGER  VERONICA      2016-09-12      40447   \n",
       "1      149612       ROTTA  JONATHAN      2013-09-16      40447   \n",
       "2      206583   WILSON II    ROBERT      2018-04-30      40447   \n",
       "3      199874  WASHINGTON    MORIAH      2019-03-18      40447   \n",
       "4       58036    KRAWCZYK    AMANDA      2017-05-15      40447   \n",
       "\n",
       "                 TitleDescription WorkLocationBorough LeaveStatusasofJune30  \n",
       "0  EMERGENCY PREPAREDNESS MANAGER            BROOKLYN                ACTIVE  \n",
       "1  EMERGENCY PREPAREDNESS MANAGER            BROOKLYN                ACTIVE  \n",
       "2  EMERGENCY PREPAREDNESS MANAGER            BROOKLYN                ACTIVE  \n",
       "3  EMERGENCY PREPAREDNESS MANAGER            BROOKLYN                ACTIVE  \n",
       "4  EMERGENCY PREPAREDNESS MANAGER            BROOKLYN                ACTIVE  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dim_Employee\n",
    "Dim_Employee_df = transformed_nycpayroll_df[[\n",
    "                \"EmployeeID\", \"LastName\", \"FirstName\", \"AgencyStartDate\", \n",
    "                \"TitleCode\", \"TitleDescription\", \"WorkLocationBorough\", \n",
    "                \"LeaveStatusasofJune30\"\n",
    "            ]]\n",
    "\n",
    "Dim_Employee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dim_Employee_df.to_sql(\"Dim_Employee\", con=conn, if_exists=\"replace\", index=False)\n",
    "logging.info('Dim_Employee data loaded successfully!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>AgencyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2120</td>\n",
       "      <td>OFFICE OF EMERGENCY MANAGEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgencyID                      AgencyName\n",
       "0      2120  OFFICE OF EMERGENCY MANAGEMENT\n",
       "1      2120  OFFICE OF EMERGENCY MANAGEMENT\n",
       "2      2120  OFFICE OF EMERGENCY MANAGEMENT\n",
       "3      2120  OFFICE OF EMERGENCY MANAGEMENT\n",
       "4      2120  OFFICE OF EMERGENCY MANAGEMENT"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Dim_Agency\n",
    "Dim_Agency_df = transformed_nycpayroll_df[[\"AgencyID\", \"AgencyName\"]]\n",
    "Dim_Agency_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dim_Agency_df.to_sql(\"Dim_Agency\", con=conn, if_exists=\"replace\", index=False)\n",
    "logging.info('Dim_Agency data loaded successfully!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PayBasisID</th>\n",
       "      <th>PayBasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>per Annum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>per Annum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>per Annum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>per Annum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>per Annum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PayBasisID   PayBasis\n",
       "0           1  per Annum\n",
       "1           1  per Annum\n",
       "2           1  per Annum\n",
       "3           1  per Annum\n",
       "4           1  per Annum"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dim_PayBasis\n",
    "dim_paybasis_df = transformed_nycpayroll_df[[\"PayBasisID\", \"PayBasis\"]]\n",
    "dim_paybasis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "dim_paybasis_df.to_sql(\"Dim_PayBasis\", con=conn, if_exists=\"replace\", index=False)\n",
    "logging.info('Dim_PayBasis data loaded successfully!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200000</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19980000</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     DateID        Date  Year  Month  Day  Quarter\n",
       "0  20200000  2020-01-01  2020      1    1        1\n",
       "1  19980000  1998-01-01  1998      1    1        1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dim_Date (from dim_date.csv)\n",
    "dim_date_df = pd.read_csv(\"Dataset/Cleaned_payroll_Data/dim_date.csv\")\n",
    "\n",
    "dim_date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date_df.to_sql(\"Dim_Date\", con=conn, if_exists=\"replace\", index=False)\n",
    "logging.info('Dim_Date data loaded successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>DateID</th>\n",
       "      <th>PayBasisID</th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>PayrollNumber</th>\n",
       "      <th>BaseSalary</th>\n",
       "      <th>RegularHours</th>\n",
       "      <th>OTHours</th>\n",
       "      <th>RegularGrossPaid</th>\n",
       "      <th>TotalOTPaid</th>\n",
       "      <th>TotalOtherPay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2120</td>\n",
       "      <td>20200000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149612</td>\n",
       "      <td>2120</td>\n",
       "      <td>20200000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206583</td>\n",
       "      <td>2120</td>\n",
       "      <td>20200000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84698.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199874</td>\n",
       "      <td>2120</td>\n",
       "      <td>20200000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87900.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3202.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58036</td>\n",
       "      <td>2120</td>\n",
       "      <td>20200000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>86005.0</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83976.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeID  AgencyID    DateID  PayBasisID  FiscalYear  PayrollNumber  \\\n",
       "0       10001      2120  20200000           1        2020             17   \n",
       "1      149612      2120  20200000           1        2020             17   \n",
       "2      206583      2120  20200000           1        2020             17   \n",
       "3      199874      2120  20200000           1        2020             17   \n",
       "4       58036      2120  20200000           1        2020             17   \n",
       "\n",
       "   BaseSalary  RegularHours  OTHours  RegularGrossPaid  TotalOTPaid  \\\n",
       "0     86005.0        1820.0      0.0          84698.21          0.0   \n",
       "1     86005.0        1820.0      0.0          84698.21          0.0   \n",
       "2     86005.0        1820.0      0.0          84698.21          0.0   \n",
       "3     86005.0        1820.0      0.0          87900.95          0.0   \n",
       "4     86005.0        1820.0      0.0          83976.54          0.0   \n",
       "\n",
       "   TotalOtherPay  \n",
       "0           0.00  \n",
       "1           0.00  \n",
       "2           0.00  \n",
       "3       -3202.74  \n",
       "4           0.00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # FactPayroll\n",
    "# Merge with dim_date_df to include DateID\n",
    "factpayroll_df = transformed_nycpayroll_df.merge(dim_date_df, left_on=\"FiscalYear\", right_on=\"Year\", how=\"inner\") \\\n",
    "                                                        [[\"EmployeeID\", \"AgencyID\", \"DateID\", \"PayBasisID\", \"FiscalYear\", \\\n",
    "                                                        \"PayrollNumber\", \"BaseSalary\", \"RegularHours\", \"OTHours\", \\\n",
    "                                                        \"RegularGrossPaid\", \"TotalOTPaid\", \"TotalOtherPay\"]]\n",
    "\n",
    "factpayroll_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeID  AgencyID    DateID  PayBasisID  FiscalYear  PayrollNumber  \\\n",
      "0       10001      2120  20200000           1        2020             17   \n",
      "1      149612      2120  20200000           1        2020             17   \n",
      "2      206583      2120  20200000           1        2020             17   \n",
      "3      199874      2120  20200000           1        2020             17   \n",
      "4       58036      2120  20200000           1        2020             17   \n",
      "\n",
      "   BaseSalary  RegularHours  OTHours  RegularGrossPaid  TotalOTPaid  \\\n",
      "0     86005.0        1820.0      0.0          84698.21          0.0   \n",
      "1     86005.0        1820.0      0.0          84698.21          0.0   \n",
      "2     86005.0        1820.0      0.0          84698.21          0.0   \n",
      "3     86005.0        1820.0      0.0          87900.95          0.0   \n",
      "4     86005.0        1820.0      0.0          83976.54          0.0   \n",
      "\n",
      "   TotalOtherPay  \n",
      "0           0.00  \n",
      "1           0.00  \n",
      "2           0.00  \n",
      "3       -3202.74  \n",
      "4           0.00  \n",
      "Number of rows in factpayroll_df: 100\n"
     ]
    }
   ],
   "source": [
    "print(factpayroll_df.head())  # Before adding FactID and reordering\n",
    "print(f\"Number of rows in factpayroll_df: {len(factpayroll_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "           \n",
    "\n",
    "# Creating the FactID after the merging\n",
    "factpayroll_df[\"FactID\"] = range(1, len(factpayroll_df) + 1)\n",
    "\n",
    "# Reorder columns to have FactID first\n",
    "factpayroll_df = factpayroll_df[[\n",
    "                \"FactID\", \"EmployeeID\", \"AgencyID\", \"DateID\", \"PayBasisID\", \"FiscalYear\", \\\n",
    "                \"PayrollNumber\", \"BaseSalary\", \"RegularHours\", \"OTHours\", \"RegularGrossPaid\", \\\n",
    "                \"TotalOTPaid\", \"TotalOtherPay\"\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FactID  EmployeeID  AgencyID    DateID  PayBasisID  FiscalYear  \\\n",
      "0       1       10001      2120  20200000           1        2020   \n",
      "1       2      149612      2120  20200000           1        2020   \n",
      "2       3      206583      2120  20200000           1        2020   \n",
      "3       4      199874      2120  20200000           1        2020   \n",
      "4       5       58036      2120  20200000           1        2020   \n",
      "\n",
      "   PayrollNumber  BaseSalary  RegularHours  OTHours  RegularGrossPaid  \\\n",
      "0             17     86005.0        1820.0      0.0          84698.21   \n",
      "1             17     86005.0        1820.0      0.0          84698.21   \n",
      "2             17     86005.0        1820.0      0.0          84698.21   \n",
      "3             17     86005.0        1820.0      0.0          87900.95   \n",
      "4             17     86005.0        1820.0      0.0          83976.54   \n",
      "\n",
      "   TotalOTPaid  TotalOtherPay  \n",
      "0          0.0           0.00  \n",
      "1          0.0           0.00  \n",
      "2          0.0           0.00  \n",
      "3          0.0       -3202.74  \n",
      "4          0.0           0.00  \n",
      "Number of rows in factpayroll_df: 100\n"
     ]
    }
   ],
   "source": [
    "print(factpayroll_df.head())  # Before adding FactID and reordering\n",
    "print(f\"Number of rows in factpayroll_df: {len(factpayroll_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factpayroll_df.to_sql(\"FactPayroll\", con=conn, if_exists=\"replace\", index=False)\n",
    "logging.info('FactPayroll data loaded successfully!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'factpayroll_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfactpayroll_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead())  \u001b[38;5;66;03m# Before adding FactID and reordering\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows in factpayroll_df: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(factpayroll_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'factpayroll_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(factpayroll_df.head())  # Before adding FactID and reordering\n",
    "print(f\"Number of rows in factpayroll_df: {len(factpayroll_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygotoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
